# Cat Facts Semantic Search Chatbot

This project demonstrates a simple semantic search chatbot using embeddings and retrieval augmented generation (RAG) with the Ollama Python SDK. It allows you to query a dataset of cat facts and get relevant responses generated by a language model based on retrieved knowledge.

---

## Features

- Load cat facts dataset from a text file
- Generate semantic embeddings for each fact using Ollama embedding model
- Store embeddings in an in-memory vector database
- Retrieve top relevant facts based on cosine similarity to query
- Generate chatbot answers grounded strictly on retrieved facts using an Ollama language model

---

## Requirements

- Python 3.8+
- [Ollama Python SDK](https://ollama.com/docs/sdk)
- Access to Ollama models:
  - Embedding model: `hf.co/CompendiumLabs/bge-base-en-v1.5-gguf`
  - Language model: `hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF`
- A text file named `cat-facts.txt` containing cat facts, one fact per line

---

## Setup

1. Clone the repository

2. Install Ollama SDK if not already installed:

   ```bash
   pip install ollama
Place your cat-facts.txt file in the project root folder.

How it works
Load cat facts dataset from cat-facts.txt

For each fact (chunk), generate an embedding vector using the Ollama embedding model.

Store each (fact, embedding) tuple in an in-memory vector database (vectorDB list).

When a user inputs a query:

Generate embedding for the query.

Calculate cosine similarity between query embedding and all stored embeddings.

Retrieve top N most similar facts.

Construct a prompt with retrieved facts as context and feed it to the Ollama language model to generate an answer grounded in the facts.

Notes
Make sure the Ollama models are correctly installed and accessible via the Ollama CLI or API.

The cat-facts.txt file should contain clean, distinct facts about cats, one per line, to maximize retrieval effectiveness.

This example uses an in-memory vector database for simplicity; for large datasets, consider persistent vector databases like FAISS or Pinecone.

The cosine similarity function can be optimized or replaced with libraries such as NumPy for better performance.

License
This project is licensed under the MIT License.

Feel free to open issues or submit pull requests to improve the project!

---







